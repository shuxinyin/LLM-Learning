{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from RAG.VectorBase import VectorStore\n",
    "from RAG.utils import ReadFiles\n",
    "from RAG.LLM import OpenAIChat\n",
    "from RAG.Embeddings import BgeEmbedding\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ChatMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = dict(\n",
    "    RAG_PROMPT_TEMPALTE=\"\"\"结合以上下文来回答用户的问题。\n",
    "        问题: {question}\n",
    "        可参考的上下文：\n",
    "        ···\n",
    "        {context}\n",
    "        ···\n",
    "        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，不要臆想推测，使用中文回答。\n",
    "        回答:\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "class MyChatOpenAI():\n",
    "    def __init__(self, path: str = '', model: str = \"Qwen2.5-7B-Instruct\",\n",
    "                 api_key: str = \"YOUR API KEY\", base_url: str = \"http://localhost:6006/v1\",\n",
    "                 temperature: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.client = ChatOpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url,\n",
    "            temperature=temperature,\n",
    "            model_name=model\n",
    "        )\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        history.append(\n",
    "            HumanMessage(content=PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content))\n",
    "        )\n",
    "        response = self.client(messages=history)\n",
    "        return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|█████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 128.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git是一种分布式版本控制系统，用于跟踪文件的变化并支持协作开发。它可以让每个开发者拥有整个代码库的历史记录，无需依赖中央服务器，并且分支和合并操作非常高效，适合大型项目的开发。\n"
     ]
    }
   ],
   "source": [
    "# 没有保存数据库\n",
    "docs = ReadFiles('./data').get_content(max_token_len=600, cover_content=150)  # 获得data目录下的所有文件内容并分割\n",
    "vector = VectorStore(docs)\n",
    "embedding = BgeEmbedding(path='D:/MyProject/TorchProject/NLP/model_hub/bge-small-zh-v1.5')  # 创建EmbeddingModel\n",
    "vector.get_vector(EmbeddingModel=embedding)\n",
    "vector.persist(path='storage')  # 将向量和文档内容保存到storage目录下，下次再用就可以直接加载本地的数据库\n",
    "\n",
    "question = '什么是git？'\n",
    "\n",
    "content = vector.query(question, EmbeddingModel=embedding, k=1)[0]\n",
    "\n",
    "chat = MyChatOpenAI()\n",
    "print(chat.chat(question, [], content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git的分支原理主要是用来隔离开发工作的。每个分支都是一个独立的开发环境，互不影响。分支可以很方便地被创建和合并，因此许多开发者使用分支来进行特性开发、修复bug或者尝试新想法。\n",
      "\n",
      "在Git中，几乎所有操作都是本地执行的，包括创建和切换分支。你可以使用以下命令来操作分支：\n",
      "\n",
      "- 创建新分支：`git branch <分支名>`\n",
      "- 切换到新分支：`git checkout <分支名>`\n",
      "- 创建并立即切换到新分支：`git checkout -b <分支名>`\n",
      "- 合并指定分支到当前分支：`git merge <分支名>`\n",
      "\n",
      "合理的分支策略可以帮助团队更有效地协作。例如，GitFlow定义了一个围绕项目发布的分支模型，包括功能分支、发布分支、维护分支等。而GitHubFlow则更加简单灵活，适合持续交付的项目。在GitHubFlow中，master分支通常是稳定的，并且随时可以部署。所有新的开发都在基于master的特性分支上进行，一旦完成就可以合并回master。\n",
      "\n",
      "如果在合并分支时遇到冲突，Git会提示你需要手动解决这些冲突。解决冲突后，可以继续完成合并操作。\n"
     ]
    }
   ],
   "source": [
    "vector = VectorStore()\n",
    "\n",
    "vector.load_vector('./storage')  # 加载本地的数据库\n",
    "\n",
    "embedding = BgeEmbedding(path='D:/MyProject/TorchProject/NLP/model_hub/bge-small-zh-v1.5')\n",
    "\n",
    "question = 'git的分支原理？'\n",
    "\n",
    "content = vector.query(question, EmbeddingModel=embedding, k=1)[0]\n",
    "\n",
    "chat = MyChatOpenAI()\n",
    "print(chat.chat(question, [], content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
